{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations on GCP with TensorFlow and WALS with Cloud Composer\n",
    "***\n",
    "This lab is adapted from the original [solution](https://github.com/GoogleCloudPlatform/tensorflow-recommendation-wals) created by [lukmanr](https://github.com/GoogleCloudPlatform/tensorflow-recommendation-wals/commits?author=lukmanr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project deploys a solution for a recommendation service on GCP, using the WALS algorithm in TensorFlow. Components include:\n",
    "\n",
    "- Recommendation model code, and scripts to train and tune the model on ML Engine\n",
    "- A REST endpoint using Google Cloud Endpoints for serving recommendations\n",
    "- An Airflow server managed by Cloud Composer for running scheduled model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Composer Instance\n",
    "- Create a Cloud Composer [instance](https://console.cloud.google.com/composer/environments/create?project=)\n",
    "    1. Specify 'composer' for name\n",
    "    2. Choose a location\n",
    "    3. Keep the remaining settings at their defaults\n",
    "    4. Select Create\n",
    "\n",
    "This takes 15 - 20 minutes. Continue with the rest of the lab as you will be using Cloud Composer near the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sh\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/22/17b22ef5b049f12080f5815c41bf94de3c229217609e469001a8f80c1b3d/sh-1.12.14-py2.py3-none-any.whl\n",
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/d8/f3/413bab4ff08e1fc4828dfc59996d721917df8e8583ea85385d51125dceff/pip-19.0.3-py2.py3-none-any.whl (1.4MB)\n",
      "Installing collected packages: sh, pip\n",
      "  Found existing installation: pip 18.1\n",
      "    Uninstalling pip-18.1:\n",
      "      Successfully uninstalled pip-18.1\n",
      "Successfully installed pip-19.0.3 sh-1.12.14\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install sh --upgrade pip # needed to execute shell scripts later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment variables\n",
    "<span style=\"color: blue\">__Replace the below settings with your own.__</span> Note: you can leave AIRFLOW_BUCKET blank and come back to it after your Composer instance is created which automatically will create an Airflow bucket for you. <br><br>\n",
    "\n",
    "### 1. Make a GCS bucket with the name recserve_[YOUR-PROJECT-ID]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-a16809804f6b9f6f' # REPLACE WITH YOUR PROJECT ID\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = 'recserve_' + PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating recserve_bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://recserve_qwiklabs-gcp-a16809804f6b9f6f/...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# create GCS bucket with recserve_PROJECT_NAME if not exists\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"Not creating recserve_bucket since it already exists.\"\n",
    "else\n",
    "   echo \"Creating recserve_bucket\"\n",
    "   gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Google App Engine permissions\n",
    "1. In [IAM](https://console.cloud.google.com/iam-admin/iam?project=), __change permissions for \"Compute Engine default service account\" from Editor to Owner__. This is required so you can create and deploy App Engine versions from within Cloud Datalab. Note: the alternative is to run all app engine commands directly in Cloud Shell instead of from within Cloud Datalab.<br/><br/>\n",
    "\n",
    "2. Create an App Engine instance if you have not already by uncommenting and running the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# run app engine creation commands\n",
    "# gcloud app create --region ${REGION} # see: https://cloud.google.com/compute/docs/regions-zones/\n",
    "# gcloud app update --no-split-health-checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Setup and Train the WALS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload sample data to BigQuery \n",
    "This tutorial comes with a sample Google Analytics data set, containing page tracking events from the Austrian news site Kurier.at. The schema file '''ga_sessions_sample_schema.json''' is located in the folder data in the tutorial code, and the data file '''ga_sessions_sample.json.gz''' is located in a public Cloud Storage bucket associated with this tutorial. To upload this data set to BigQuery:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy sample data files into our bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/ga_sessions_sample.json.gz [Content-Type=application/json]...\n",
      "/ [0/1 files][    0.0 B/121.3 MiB]   0% Done                                    \r",
      "/ [0/1 files][121.3 MiB/121.3 MiB]  99% Done                                    \r",
      "-\r",
      "- [1/1 files][121.3 MiB/121.3 MiB] 100% Done                                    \r\n",
      "Operation completed over 1 objects/121.3 MiB.                                    \n",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/recommendation_events.csv...\n",
      "/ [0/1 files][    0.0 B/ 10.0 MiB]   0% Done                                    \r",
      "/ [1/1 files][ 10.0 MiB/ 10.0 MiB] 100% Done                                    \r\n",
      "Operation completed over 1 objects/10.0 MiB.                                     \n",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/recommendation_events.csv [Content-Type=text/csv]...\n",
      "/ [0/1 files][    0.0 B/ 10.0 MiB]   0% Done                                    \r",
      "-\r",
      "- [1/1 files][ 10.0 MiB/ 10.0 MiB] 100% Done                                    \r\n",
      "Operation completed over 1 objects/10.0 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/ga_sessions_sample.json.gz gs://${BUCKET}/data/ga_sessions_sample.json.gz\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/recommendation_events.csv data/recommendation_events.csv\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/endtoend/data/recommendation_events.csv gs://${BUCKET}/data/recommendation_events.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create empty BigQuery dataset and load sample JSON data\n",
    "Note: Ingesting the 400K rows of sample data. This usually takes 5-7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GA360_test dataset.\n",
      "Dataset 'qwiklabs-gcp-a16809804f6b9f6f:GA360_test' successfully created.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (0s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (1s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (2s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (3s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (4s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (5s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (6s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (7s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (8s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (9s) Current status: RUNNING\r",
      "                                                                                      \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (10s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (11s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (12s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (13s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (14s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (16s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (17s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (18s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (19s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (20s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (21s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (22s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (23s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (24s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (25s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (26s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (27s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (28s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (29s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (30s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (31s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (32s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (33s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (34s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (35s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (36s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (37s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (38s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (39s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (40s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (41s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (42s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (43s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (44s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (45s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (46s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (47s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (48s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (49s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (50s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (51s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (52s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (53s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (54s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (55s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (56s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (57s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (58s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (59s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (60s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (61s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (62s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (63s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (64s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (65s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (66s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (67s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (68s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (69s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (70s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (71s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (72s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (73s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (74s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (75s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (76s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (77s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (78s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (79s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (80s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (81s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (82s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (83s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (84s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (85s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (86s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (87s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (88s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (89s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (90s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (91s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (92s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (93s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (94s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (95s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (96s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (97s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (98s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (99s) Current status: RUNNING\r",
      "                                                                                       \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (100s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (101s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (102s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (103s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (104s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (105s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (106s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (107s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (108s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (109s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (110s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (111s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (112s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (113s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (114s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (115s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (116s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (117s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (118s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (119s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (120s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (121s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (122s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (123s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (124s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (125s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (126s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (127s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (128s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (129s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (130s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (131s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (132s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (133s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (134s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (135s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (136s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (137s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (138s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (139s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (140s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (141s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (142s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (143s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (144s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (145s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (146s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (147s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (148s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (149s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (150s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (151s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (152s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (153s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (154s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (155s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (156s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (157s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (158s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (159s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (160s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (161s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (162s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (163s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (164s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (165s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (166s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (167s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (168s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (169s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (170s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (171s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (172s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (173s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (174s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (175s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (176s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (177s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (178s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (179s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (180s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (181s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (182s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (183s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (184s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (185s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (186s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (187s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (188s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (189s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (190s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (191s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (192s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (193s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (195s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (196s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (197s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (198s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (199s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (200s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (201s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (202s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (203s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (204s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (205s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (206s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (207s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (208s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (209s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (210s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (211s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (212s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (213s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (214s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (215s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (216s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (217s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (218s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (219s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (220s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (221s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (222s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (223s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (224s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (225s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (226s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (227s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (228s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (229s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (230s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (231s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (232s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (233s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (234s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (235s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (236s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (237s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (238s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (239s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (240s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (241s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (242s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (243s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (244s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (245s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (246s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (247s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (248s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (249s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (250s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (251s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (252s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (253s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (254s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (255s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (256s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (257s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (258s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (259s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (260s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (261s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (262s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (263s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (264s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (265s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (266s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (267s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (268s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (269s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (270s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (271s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (272s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (273s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (274s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (275s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (276s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (277s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (278s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (279s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (280s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (281s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (282s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (283s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (284s) Current status: RUNNING\r",
      "                                                                                        \r",
      "Waiting on bqjob_r3ec8b038a230137f_00000169642b451a_1 ... (284s) Current status: DONE   "
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# create BigQuery dataset if it doesn't already exist\n",
    "exists=$(bq ls -d | grep -w GA360_test)\n",
    "if [ -n \"$exists\" ]; then\n",
    "   echo \"Not creating GA360_test since it already exists.\"\n",
    "else\n",
    "   echo \"Creating GA360_test dataset.\"\n",
    "   bq --project_id=${PROJECT} mk GA360_test \n",
    "fi\n",
    "\n",
    "# create the schema and load our sample Google Analytics session data\n",
    "bq load --source_format=NEWLINE_DELIMITED_JSON \\\n",
    " GA360_test.ga_sessions_sample \\\n",
    " gs://${BUCKET}/data/ga_sessions_sample.json.gz \\\n",
    " data/ga_sessions_sample_schema.json # can't load schema files from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install WALS model training package and model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a distributable package. Copy the package up to the code folder in the bucket you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating distributable package\n",
      "running sdist\n",
      "running egg_info\n",
      "creating wals_ml_engine.egg-info\n",
      "writing requirements to wals_ml_engine.egg-info/requires.txt\n",
      "writing wals_ml_engine.egg-info/PKG-INFO\n",
      "writing top-level names to wals_ml_engine.egg-info/top_level.txt\n",
      "writing dependency_links to wals_ml_engine.egg-info/dependency_links.txt\n",
      "writing manifest file 'wals_ml_engine.egg-info/SOURCES.txt'\n",
      "reading manifest file 'wals_ml_engine.egg-info/SOURCES.txt'\n",
      "writing manifest file 'wals_ml_engine.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating wals_ml_engine-0.1\n",
      "creating wals_ml_engine-0.1/trainer\n",
      "creating wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying files to wals_ml_engine-0.1...\n",
      "copying README.md -> wals_ml_engine-0.1\n",
      "copying setup.py -> wals_ml_engine-0.1\n",
      "copying trainer/__init__.py -> wals_ml_engine-0.1/trainer\n",
      "copying trainer/model.py -> wals_ml_engine-0.1/trainer\n",
      "copying trainer/task.py -> wals_ml_engine-0.1/trainer\n",
      "copying trainer/util.py -> wals_ml_engine-0.1/trainer\n",
      "copying trainer/wals.py -> wals_ml_engine-0.1/trainer\n",
      "copying wals_ml_engine.egg-info/PKG-INFO -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying wals_ml_engine.egg-info/SOURCES.txt -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying wals_ml_engine.egg-info/dependency_links.txt -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying wals_ml_engine.egg-info/requires.txt -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "copying wals_ml_engine.egg-info/top_level.txt -> wals_ml_engine-0.1/wals_ml_engine.egg-info\n",
      "Writing wals_ml_engine-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'wals_ml_engine-0.1' (and everything under it)\n",
      "copying ML package to bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "Copying file://dist/wals_ml_engine-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [0 files][    0.0 B/  8.3 KiB]                                                \r",
      "/ [1 files][  8.3 KiB/  8.3 KiB]                                                \r\n",
      "Operation completed over 1 objects/8.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd wals_ml_engine\n",
    "\n",
    "echo \"creating distributable package\"\n",
    "python setup.py sdist\n",
    "\n",
    "echo \"copying ML package to bucket\"\n",
    "gsutil cp dist/wals_ml_engine-0.1.tar.gz gs://${BUCKET}/code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run the WALS model on the sample data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2017 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "\n",
      "usage () {\n",
      "  echo \"usage: mltrain.sh [local | train | tune] [gs://]<input_file>.csv\n",
      "                  [--data-type ratings|web_views]\n",
      "                  [--delimiter <delim>]\n",
      "                  [--use-optimized]\n",
      "                  [--headers]\n",
      "\n",
      "Use 'local' to train locally with a local data file, and 'train' and 'tune' to\n",
      "run on ML Engine.  For ML Engine jobs the input file must reside on GCS.\n",
      "\n",
      "Optional args:\n",
      "  --data-type:      Default to 'ratings', meaning MovieLens ratings from 0-5.\n",
      "                    Set to 'web_views' for Google Analytics data.\n",
      "  --delimiter:      CSV delimiter, default to '\\t'.\n",
      "  --use-optimized:  Use optimized hyperparamters, default False.\n",
      "  --headers:        Default False for 'ratings', True for 'web_views'.\n",
      "\n",
      "Examples:\n",
      "\n",
      "# train locally with unoptimized hyperparams\n",
      "./mltrain.sh local ../data/recommendation_events.csv --data-type web_views\n",
      "\n",
      "# train on ML Engine with optimized hyperparams\n",
      "./mltrain.sh train gs://rec_serve/data/recommendation_events.csv --data-type web_views --use-optimized\n",
      "\n",
      "# tune hyperparams on ML Engine:\n",
      "./mltrain.sh tune gs://rec_serve/data/recommendation_events.csv --data-type web_views\n",
      "\"\n",
      "\n",
      "}\n",
      "\n",
      "date\n",
      "\n",
      "TIME=`date +\"%Y%m%d_%H%M%S\"`\n",
      "\n",
      "# CHANGE TO YOUR BUCKET\n",
      "BUCKET=\"gs://recserve_qwiklabs-gcp-a16809804f6b9f6f\"\n",
      "\n",
      "if [[ $# < 2 ]]; then\n",
      "  usage\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "# set job vars\n",
      "TRAIN_JOB=\"$1\"\n",
      "TRAIN_FILE=\"$2\"\n",
      "JOB_NAME=wals_ml_${TRAIN_JOB}_${TIME}\n",
      "REGION=us-central1\n",
      "\n",
      "# add additional args\n",
      "shift; shift\n",
      "ARGS=\"--train-files ${TRAIN_FILE} --verbose-logging $@\"\n",
      "\n",
      "if [[ ${TRAIN_JOB} == \"local\" ]]; then\n",
      "\n",
      "  mkdir -p jobs/${JOB_NAME}\n",
      "\n",
      "  gcloud ml-engine local train \\\n",
      "    --module-name trainer.task \\\n",
      "    --package-path trainer \\\n",
      "    -- \\\n",
      "    --job-dir jobs/${JOB_NAME} \\\n",
      "    ${ARGS}\n",
      "\n",
      "elif [[ ${TRAIN_JOB} == \"train\" ]]; then\n",
      "\n",
      "  gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
      "    --region $REGION \\\n",
      "    --scale-tier=CUSTOM \\\n",
      "    --job-dir ${BUCKET}/jobs/${JOB_NAME} \\\n",
      "    --module-name trainer.task \\\n",
      "    --package-path trainer \\\n",
      "    --config trainer/config/config_train.json \\\n",
      "    -- \\\n",
      "    ${ARGS}\n",
      "\n",
      "elif [[ $TRAIN_JOB == \"tune\" ]]; then\n",
      "\n",
      "  # set configuration for tuning\n",
      "  CONFIG_TUNE=\"trainer/config/config_tune.json\"\n",
      "  for i in $ARGS ; do\n",
      "    if [[ \"$i\" == \"web_views\" ]]; then\n",
      "      CONFIG_TUNE=\"trainer/config/config_tune_web.json\"\n",
      "      break\n",
      "    fi\n",
      "  done\n",
      "\n",
      "  gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
      "    --region ${REGION} \\\n",
      "    --scale-tier=CUSTOM \\\n",
      "    --job-dir ${BUCKET}/jobs/${JOB_NAME} \\\n",
      "    --module-name trainer.task \\\n",
      "    --package-path trainer \\\n",
      "    --config ${CONFIG_TUNE} \\\n",
      "    -- \\\n",
      "    --hypertune \\\n",
      "    ${ARGS}\n",
      "\n",
      "else\n",
      "  usage\n",
      "fi\n",
      "\n",
      "date\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# view the ML train local script before running\n",
    "cat wals_ml_engine/mltrain.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  9 20:42:47 UTC 2019\n",
      "Sat Mar  9 20:44:20 UTC 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Train Start: 2019-03-09 20:43:34\n",
      "trainer/wals.py:94: RuntimeWarning: divide by zero encountered in divide\n",
      "  frac = np.array(1.0/(data > 0.0).sum(axis))\n",
      "2019-03-09 20:43:37.873257: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-03-09 20:43:39.489093: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:41.679153: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:43.569240: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:45.427972: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:47.290564: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:49.176274: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:51.066807: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:52.896135: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:54.706254: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:56.553311: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:43:58.404524: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:00.277910: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:02.134797: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:04.013632: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:05.838098: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:07.734194: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:09.603605: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:11.473521: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:13.334945: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "2019-03-09 20:44:15.170004: W tensorflow/core/framework/allocator.cc:101] Allocation of 277354800 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Train Finish: 2019-03-09 20:44:17\n",
      "INFO:tensorflow:train RMSE = 258540.50\n",
      "INFO:tensorflow:test RMSE = 313425.46\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd wals_ml_engine\n",
    "\n",
    "# train locally with unoptimized hyperparams\n",
    "./mltrain.sh local ../data/recommendation_events.csv --data-type web_views --use-optimized\n",
    "\n",
    "# Options if we wanted to train on CMLE. We will do this with Cloud Composer later\n",
    "# train on ML Engine with optimized hyperparams\n",
    "# ./mltrain.sh train ../data/recommendation_events.csv --data-type web_views --use-optimized\n",
    "\n",
    "# tune hyperparams on ML Engine:\n",
    "# ./mltrain.sh tune ../data/recommendation_events.csv --data-type web_views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a couple minutes, and create a job directory under wals_ml_engine/jobs like \"wals_ml_local_20180102_012345/model\", containing the model files saved as numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the locally trained model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mwals_ml_local_20190309_204247\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls wals_ml_engine/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Copy the model files from this directory to the model folder in the project bucket:\n",
    "In the case of multiple models, take the most recent (tail -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation model file numpy arrays in bucket:\n",
      "gs://recserve_qwiklabs-gcp-a16809804f6b9f6f/model/col.npy\n",
      "gs://recserve_qwiklabs-gcp-a16809804f6b9f6f/model/item.npy\n",
      "gs://recserve_qwiklabs-gcp-a16809804f6b9f6f/model/row.npy\n",
      "gs://recserve_qwiklabs-gcp-a16809804f6b9f6f/model/user.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://wals_ml_engine/jobs/wals_ml_local_20190309_204247/model/col.npy [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/642.1 KiB]                                                \r",
      "/ [1 files][642.1 KiB/642.1 KiB]                                                \r",
      "Copying file://wals_ml_engine/jobs/wals_ml_local_20190309_204247/model/item.npy [Content-Type=application/octet-stream]...\n",
      "/ [1 files][642.1 KiB/685.0 KiB]                                                \r",
      "/ [2 files][685.0 KiB/685.0 KiB]                                                \r",
      "Copying file://wals_ml_engine/jobs/wals_ml_local_20190309_204247/model/row.npy [Content-Type=application/octet-stream]...\n",
      "/ [2 files][685.0 KiB/  9.9 MiB]                                                \r",
      "/ [3 files][  9.9 MiB/  9.9 MiB]                                                \r",
      "-\r",
      "Copying file://wals_ml_engine/jobs/wals_ml_local_20190309_204247/model/user.npy [Content-Type=application/octet-stream]...\n",
      "- [3 files][  9.9 MiB/ 10.5 MiB]                                                \r",
      "- [4 files][ 10.5 MiB/ 10.5 MiB]                                                \r\n",
      "Operation completed over 4 objects/10.5 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "export JOB_MODEL=$(find wals_ml_engine/jobs -name \"model\" | tail -1)\n",
    "gsutil cp ${JOB_MODEL}/* gs://${BUCKET}/model/\n",
    "  \n",
    "echo \"Recommendation model file numpy arrays in bucket:\"  \n",
    "gsutil ls gs://${BUCKET}/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the recserve endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the deploy template for the Cloud Endpoint API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "# Copyright 2017 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "set -euo pipefail\n",
      "\n",
      "source util.sh\n",
      "\n",
      "main() {\n",
      "  # Get our working project, or exit if it's not set.\n",
      "  local project_id=$(get_project_id)\n",
      "  if [[ -z \"$project_id\" ]]; then\n",
      "    exit 1\n",
      "  fi\n",
      "  local temp_file=$(mktemp)\n",
      "  export TEMP_FILE=\"${temp_file}.yaml\"\n",
      "  mv \"$temp_file\" \"$TEMP_FILE\"\n",
      "\n",
      "  # Because the included API is a template, we have to do some string\n",
      "  # substitution before we can deploy it. Sed does this nicely.\n",
      "  < \"$API_FILE\" sed -E \"s/YOUR-PROJECT-ID/${project_id}/g\" > \"$TEMP_FILE\"\n",
      "  echo \"Preparing config for deploying service in $API_FILE...\"\n",
      "  echo \"To deploy:  gcloud endpoints services deploy $TEMP_FILE\"\n",
      "}\n",
      "\n",
      "# Defaults.\n",
      "API_FILE=\"../app/openapi.yaml\"\n",
      "\n",
      "if [[ \"$#\" == 0 ]]; then\n",
      "  : # Use defaults.\n",
      "elif [[ \"$#\" == 1 ]]; then\n",
      "  API_FILE=\"$1\"\n",
      "else\n",
      "  echo \"Wrong number of arguments specified.\"\n",
      "  echo \"Usage: deploy_api.sh [api-file]\"\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "main \"$@\"\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "cd scripts\n",
    "cat prepare_deploy_api.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copy and run the deploy script generated below:\n",
      "Preparing config for deploying service in ../app/openapi.yaml...\n",
      "To deploy:  gcloud endpoints services deploy /tmp/tmp.vNKM97GJVl.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "printf \"\\nCopy and run the deploy script generated below:\\n\"\n",
    "cd scripts\n",
    "./prepare_deploy_api.sh                         # Prepare config file for the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output somthing like:\n",
    "\n",
    "```To deploy:  gcloud endpoints services deploy /var/folders/1m/r3slmhp92074pzdhhfjvnw0m00dhhl/T/tmp.n6QVl5hO.yaml```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run the endpoints deploy command output above:\n",
    "<span style=\"color: blue\">Be sure to __replace the below [FILE_NAME]__ with the results from above before running.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for async operation operations/serviceConfigs.qwiklabs-gcp-a16809804f6b9f6f.appspot.com:29412742-215e-42bb-ae11-118dc4b67266 to complete...\n",
      "Operation finished successfully. The following command can describe the Operation details:\n",
      " gcloud endpoints operations describe operations/serviceConfigs.qwiklabs-gcp-a16809804f6b9f6f.appspot.com:29412742-215e-42bb-ae11-118dc4b67266\n",
      "\n",
      "Waiting for async operation operations/rollouts.qwiklabs-gcp-a16809804f6b9f6f.appspot.com:40134064-0f56-4fbb-a9e4-b3e1af0648e4 to complete...\n",
      "Operation finished successfully. The following command can describe the Operation details:\n",
      " gcloud endpoints operations describe operations/rollouts.qwiklabs-gcp-a16809804f6b9f6f.appspot.com:40134064-0f56-4fbb-a9e4-b3e1af0648e4\n",
      "\n",
      "Enabling service [endpoints.googleapis.com] on project [qwiklabs-gcp-a16809804f6b9f6f]...\n",
      "Waiting for async operation operations/acf.1850b39c-281b-414e-a930-49ff33794dd7 to complete...\n",
      "Operation finished successfully. The following command can describe the Operation details:\n",
      " gcloud services operations describe operations/tmo-acf.1850b39c-281b-414e-a930-49ff33794dd7\n",
      "\n",
      "\n",
      "Enabling service [qwiklabs-gcp-a16809804f6b9f6f.appspot.com] on project [qwiklabs-gcp-a16809804f6b9f6f]...\n",
      "Waiting for async operation operations/acf.c0f7336d-5d25-46c0-b71e-526b96ca7630 to complete...\n",
      "Operation finished successfully. The following command can describe the Operation details:\n",
      " gcloud services operations describe operations/tmo-acf.c0f7336d-5d25-46c0-b71e-526b96ca7630\n",
      "\n",
      "\n",
      "Service Configuration [2019-03-09r0] uploaded for service [qwiklabs-gcp-a16809804f6b9f6f.appspot.com]\n",
      "\n",
      "To manage your API, go to: https://console.cloud.google.com/endpoints/api/qwiklabs-gcp-a16809804f6b9f6f.appspot.com/overview?project=qwiklabs-gcp-a16809804f6b9f6f\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud endpoints services deploy /tmp/tmp.vNKM97GJVl.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the deploy template for the App Engine App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "# Copyright 2017 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "set -euo pipefail\n",
      "\n",
      "source util.sh\n",
      "\n",
      "main() {\n",
      "  # Get our working project, or exit if it's not set.\n",
      "  local project_id=\"$(get_project_id)\"\n",
      "  if [[ -z \"$project_id\" ]]; then\n",
      "    exit 1\n",
      "  fi\n",
      "  # Try to create an App Engine project in our selected region.\n",
      "  # If it already exists, return a success (\"|| true\").\n",
      "  echo \"gcloud app create --region=$REGION\"\n",
      "  gcloud app create --region=\"$REGION\" || true\n",
      "\n",
      "  # Prepare the necessary variables for substitution in our app configuration\n",
      "  # template, and create a temporary file to hold the templatized version.\n",
      "  local service_name=\"${project_id}.appspot.com\"\n",
      "  local config_id=$(get_latest_config_id \"$service_name\")\n",
      "  export TEMP_FILE=\"${APP}_deploy.yaml\"\n",
      "  < \"$APP\" \\\n",
      "    sed -E \"s/SERVICE_NAME/${service_name}/g\" \\\n",
      "    | sed -E \"s/SERVICE_CONFIG_ID/${config_id}/g\" \\\n",
      "    > \"$TEMP_FILE\"\n",
      "\n",
      "  echo \"To deploy:  gcloud -q app deploy $TEMP_FILE\"\n",
      "}\n",
      "\n",
      "# Defaults.\n",
      "APP=\"../app/app_template.yaml\"\n",
      "REGION=\"us-east1\"\n",
      "SERVICE_NAME=\"default\"\n",
      "\n",
      "if [[ \"$#\" == 0 ]]; then\n",
      "  : # Use defaults.\n",
      "elif [[ \"$#\" == 1 ]]; then\n",
      "  APP=\"$1\"\n",
      "elif [[ \"$#\" == 2 ]]; then\n",
      "  APP=\"$1\"\n",
      "  REGION=\"$2\"\n",
      "else\n",
      "  echo \"Wrong number of arguments specified.\"\n",
      "  echo \"Usage: deploy_app.sh [app-template] [region]\"\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "main \"$@\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# view the app deployment script\n",
    "cat scripts/prepare_deploy_app.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcloud app create --region=us-east1\n",
      "To deploy:  gcloud -q app deploy ../app/app_template.yaml_deploy.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are creating an app for project [qwiklabs-gcp-a16809804f6b9f6f].\n",
      "WARNING: Creating an App Engine application for a project is irreversible and the region\n",
      "cannot be changed. More information about regions is at\n",
      "<https://cloud.google.com/appengine/docs/locations>.\n",
      "\n",
      "ERROR: (gcloud.app.create) The project [qwiklabs-gcp-a16809804f6b9f6f] already contains an App Engine application. You can deploy your application using `gcloud app deploy`.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# prepare to deploy \n",
    "cd scripts\n",
    "\n",
    "./prepare_deploy_app.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ignore the script output \"ERROR: (gcloud.app.create) The project [...] already contains an App Engine application. You can deploy your application using gcloud app deploy.\" This is expected.\n",
    "\n",
    "The script will output something like:\n",
    "\n",
    "```To deploy:  gcloud -q app deploy app/app_template.yaml_deploy.yaml```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the command above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Services to deploy:\n",
      "\n",
      "descriptor:      [/content/datalab/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/endtoend/app/app_template.yaml_deploy.yaml]\n",
      "source:          [/content/datalab/training-data-analyst/courses/machine_learning/deepdive/10_recommend/labs/endtoend/app]\n",
      "target project:  [qwiklabs-gcp-a16809804f6b9f6f]\n",
      "target service:  [default]\n",
      "target version:  [20190309t205444]\n",
      "target url:      [https://qwiklabs-gcp-a16809804f6b9f6f.appspot.com]\n",
      "\n",
      "\n",
      "Enabling service [appengineflex.googleapis.com] on project [qwiklabs-gcp-a16809804f6b9f6f]...\n",
      "Waiting for async operation operations/acf.2f023152-5d18-4cde-8ab5-b92673c42f46 to complete...\n",
      "Operation finished successfully. The following command can describe the Operation details:\n",
      " gcloud services operations describe operations/tmo-acf.2f023152-5d18-4cde-8ab5-b92673c42f46\n",
      "Beginning deployment of service [default]...\n",
      "Building and pushing image for service [default]\n",
      "Started cloud build [478eb051-9cb3-43f8-a454-6b89c4f8eb4b].\n",
      "To see logs in the Cloud Console: https://console.cloud.google.com/gcr/builds/478eb051-9cb3-43f8-a454-6b89c4f8eb4b?project=1015242397328\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"478eb051-9cb3-43f8-a454-6b89c4f8eb4b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://staging.qwiklabs-gcp-a16809804f6b9f6f.appspot.com/us.gcr.io/qwiklabs-gcp-a16809804f6b9f6f/appengine/default.20190309t205444:latest#1552164904094307\n",
      "Copying gs://staging.qwiklabs-gcp-a16809804f6b9f6f.appspot.com/us.gcr.io/qwiklabs-gcp-a16809804f6b9f6f/appengine/default.20190309t205444:latest#1552164904094307...\n",
      "/ [0 files][    0.0 B/  3.3 KiB]                                                \r",
      "/ [1 files][  3.3 KiB/  3.3 KiB]                                                \r\n",
      "Operation completed over 1 objects/3.3 KiB.                                      \n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Pulling image: gcr.io/gcp-runtimes/python/gen-dockerfile@sha256:c5882736b0e7b2c32afe63c6652aa448f8f337718ef3e60388b0b8b3f5305395\n",
      "Step #0: sha256:c5882736b0e7b2c32afe63c6652aa448f8f337718ef3e60388b0b8b3f5305395: Pulling from gcp-runtimes/python/gen-dockerfile\n",
      "Step #0: Digest: sha256:c5882736b0e7b2c32afe63c6652aa448f8f337718ef3e60388b0b8b3f5305395\n",
      "Step #0: Status: Downloaded newer image for gcr.io/gcp-runtimes/python/gen-dockerfile@sha256:c5882736b0e7b2c32afe63c6652aa448f8f337718ef3e60388b0b8b3f5305395\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Pulling image: gcr.io/cloud-builders/docker@sha256:b1438be36a14c094290298d765249d71e0f2c43bc0b5565548be8fa99e870c8d\n",
      "Step #1: sha256:b1438be36a14c094290298d765249d71e0f2c43bc0b5565548be8fa99e870c8d: Pulling from cloud-builders/docker\n",
      "Step #1: 75f546e73d8b: Already exists\n",
      "Step #1: 0f3bb76fc390: Already exists\n",
      "Step #1: 3c2cba919283: Already exists\n",
      "Step #1: 773ae09605dd: Pulling fs layer\n",
      "Step #1: 773ae09605dd: Verifying Checksum\n",
      "Step #1: 773ae09605dd: Download complete\n",
      "Step #1: 773ae09605dd: Pull complete\n",
      "Step #1: Digest: sha256:b1438be36a14c094290298d765249d71e0f2c43bc0b5565548be8fa99e870c8d\n",
      "Step #1: Status: Downloaded newer image for gcr.io/cloud-builders/docker@sha256:b1438be36a14c094290298d765249d71e0f2c43bc0b5565548be8fa99e870c8d\n",
      "Step #1: Sending build context to Docker daemon  19.46kB\r",
      "\r\n",
      "Step #1: Step 1/9 : FROM gcr.io/google-appengine/python@sha256:97f8682749eb8a2c7cec2b8bfe2cd4e1a6c0dcd3af77843e1b2b3291d57859c2\n",
      "Step #1: sha256:97f8682749eb8a2c7cec2b8bfe2cd4e1a6c0dcd3af77843e1b2b3291d57859c2: Pulling from google-appengine/python\n",
      "Step #1: Digest: sha256:97f8682749eb8a2c7cec2b8bfe2cd4e1a6c0dcd3af77843e1b2b3291d57859c2\n",
      "Step #1: Status: Downloaded newer image for gcr.io/google-appengine/python@sha256:97f8682749eb8a2c7cec2b8bfe2cd4e1a6c0dcd3af77843e1b2b3291d57859c2\n",
      "Step #1:  ---> b1f5890a55cd\n",
      "Step #1: Step 2/9 : LABEL python_version=python3.6\n",
      "Step #1:  ---> Running in aa213e410baf\n",
      "Step #1: Removing intermediate container aa213e410baf\n",
      "Step #1:  ---> e69900b60e54\n",
      "Step #1: Step 3/9 : RUN virtualenv --no-download /env -p python3.6\n",
      "Step #1:  ---> Running in e7c0e90795d0\n",
      "Step #1: Running virtualenv with interpreter /opt/python3.6/bin/python3.6\n",
      "Step #1: Using base prefix '/opt/python3.6'\n",
      "Step #1: New python executable in /env/bin/python3.6\n",
      "Step #1: Also creating executable in /env/bin/python\n",
      "Step #1: Installing setuptools, pip, wheel...done.\n",
      "Step #1: Removing intermediate container e7c0e90795d0\n",
      "Step #1:  ---> c9727b119b13\n",
      "Step #1: Step 4/9 : ENV VIRTUAL_ENV /env\n",
      "Step #1:  ---> Running in 3d0a09ad35b2\n",
      "Step #1: Removing intermediate container 3d0a09ad35b2\n",
      "Step #1:  ---> 044c1e5e88c1\n",
      "Step #1: Step 5/9 : ENV PATH /env/bin:$PATH\n",
      "Step #1:  ---> Running in 4dc199a04951\n",
      "Step #1: Removing intermediate container 4dc199a04951\n",
      "Step #1:  ---> 80513dfe8272\n",
      "Step #1: Step 6/9 : ADD requirements.txt /app/\n",
      "Step #1:  ---> 10fcc004673e\n",
      "Step #1: Step 7/9 : RUN pip install -r requirements.txt\n",
      "Step #1:  ---> Running in 6376b4c2209b\n",
      "Step #1: Collecting flask (from -r requirements.txt (line 1))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/7f/e7/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n",
      "Step #1: Collecting gunicorn (from -r requirements.txt (line 2))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\n",
      "Step #1: Collecting pandas (from -r requirements.txt (line 3))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/e6/de/a0d3defd8f338eaf53ef716e40ef6d6c277c35d50e09b586e170169cdf0d/pandas-0.24.1-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "Step #1: Collecting numpy (from -r requirements.txt (line 4))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "Step #1: Collecting google-cloud-storage==1.6.0 (from -r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/c8/13/131c4d6b72411bcd56ab82a70a256d961e8d87e7b6356c12791c0003765d/google_cloud_storage-1.6.0-py2.py3-none-any.whl (51kB)\n",
      "Step #1: Collecting click>=5.1 (from flask->-r requirements.txt (line 1))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "Step #1: Collecting Werkzeug>=0.14 (from flask->-r requirements.txt (line 1))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
      "Step #1: Collecting itsdangerous>=0.24 (from flask->-r requirements.txt (line 1))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Step #1: Collecting Jinja2>=2.10 (from flask->-r requirements.txt (line 1))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/7f/ff/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731/Jinja2-2.10-py2.py3-none-any.whl (126kB)\n",
      "Step #1: Collecting pytz>=2011k (from pandas->-r requirements.txt (line 3))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/61/28/1d3920e4d1d50b19bc5d24398a7cd85cc7b9a75a490570d5a30c57622d34/pytz-2018.9-py2.py3-none-any.whl (510kB)\n",
      "Step #1: Collecting python-dateutil>=2.5.0 (from pandas->-r requirements.txt (line 3))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
      "Step #1: Collecting google-cloud-core<0.29dev,>=0.28.0 (from google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/0f/41/ae2418b4003a14cf21c1c46d61d1b044bf02cf0f8f91598af572b9216515/google_cloud_core-0.28.1-py2.py3-none-any.whl\n",
      "Step #1: Collecting google-auth>=1.0.0 (from google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/c5/9b/ed0516cc1f7609fb0217e3057ff4f0f9f3e3ce79a369c6af4a6c5ca25664/google_auth-1.6.3-py2.py3-none-any.whl (73kB)\n",
      "Step #1: Collecting google-resumable-media>=0.3.1 (from google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/e2/5d/4bc5c28c252a62efe69ed1a1561da92bd5af8eca0cdcdf8e60354fae9b29/google_resumable_media-0.3.2-py2.py3-none-any.whl\n",
      "Step #1: Collecting google-api-core<0.2.0dev,>=0.1.1 (from google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/10/65/6237293db4fbf6f0bcf7c2b67c63e4dc4837c631f194064ae84957cd0313/google_api_core-0.1.4-py2.py3-none-any.whl (50kB)\n",
      "Step #1: Collecting requests>=2.18.0 (from google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "Step #1: Collecting MarkupSafe>=0.23 (from Jinja2>=2.10->flask->-r requirements.txt (line 1))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Step #1: Collecting six>=1.5 (from python-dateutil>=2.5.0->pandas->-r requirements.txt (line 3))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Step #1: Collecting cachetools>=2.0.0 (from google-auth>=1.0.0->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/39/2b/d87fc2369242bd743883232c463f28205902b8579cb68dcf5b11eee1652f/cachetools-3.1.0-py2.py3-none-any.whl\n",
      "Step #1: Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.0->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/da/98/8ddd9fa4d84065926832bcf2255a2b69f1d03330aa4d1c49cc7317ac888e/pyasn1_modules-0.2.4-py2.py3-none-any.whl (66kB)\n",
      "Step #1: Collecting rsa>=3.1.4 (from google-auth>=1.0.0->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Step #1: Collecting googleapis-common-protos<2.0dev,>=1.5.3 (from google-api-core<0.2.0dev,>=0.1.1->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/21/fd/0ea06fab3651857955f2240a20cf951a29c1cdfdc937c3d19d8575651a64/googleapis-common-protos-1.5.8.tar.gz\n",
      "Step #1: Requirement already satisfied: setuptools>=34.0.0 in /env/lib/python3.6/site-packages (from google-api-core<0.2.0dev,>=0.1.1->google-cloud-storage==1.6.0->-r requirements.txt (line 5)) (39.1.0)\n",
      "Step #1: Collecting protobuf>=3.0.0 (from google-api-core<0.2.0dev,>=0.1.1->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/c5/60/ca38e967360212ddbb004141a70f5f6d47296e1fba37964d8ac6cb631921/protobuf-3.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "Step #1: Collecting certifi>=2017.4.17 (from requests>=2.18.0->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/60/75/f692a584e85b7eaba0e03827b3d51f45f571c2e793dd731e598828d380aa/certifi-2019.3.9-py2.py3-none-any.whl (158kB)\n",
      "Step #1: Collecting urllib3<1.25,>=1.21.1 (from requests>=2.18.0->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/62/00/ee1d7de624db8ba7090d1226aebefab96a2c71cd5cfa7629d6ad3f61b79e/urllib3-1.24.1-py2.py3-none-any.whl (118kB)\n",
      "Step #1: Collecting idna<2.9,>=2.5 (from requests>=2.18.0->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "Step #1: Collecting chardet<3.1.0,>=3.0.2 (from requests>=2.18.0->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Step #1: Collecting pyasn1<0.5.0,>=0.4.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.0->google-cloud-storage==1.6.0->-r requirements.txt (line 5))\n",
      "Step #1:   Downloading https://files.pythonhosted.org/packages/7b/7c/c9386b82a25115cccf1903441bba3cbadcfae7b678a20167347fa8ded34c/pyasn1-0.4.5-py2.py3-none-any.whl (73kB)\n",
      "Step #1: Building wheels for collected packages: googleapis-common-protos\n",
      "Step #1:   Running setup.py bdist_wheel for googleapis-common-protos: started\n",
      "Step #1:   Running setup.py bdist_wheel for googleapis-common-protos: finished with status 'done'\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/51/ef/76/6ca53de13cc85eea40c641d75bb01355d97181ed64eb67bc8e\n",
      "Step #1: Successfully built googleapis-common-protos\n",
      "Step #1: Installing collected packages: click, Werkzeug, itsdangerous, MarkupSafe, Jinja2, flask, gunicorn, numpy, pytz, six, python-dateutil, pandas, certifi, urllib3, idna, chardet, requests, protobuf, googleapis-common-protos, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-api-core, google-cloud-core, google-resumable-media, google-cloud-storage\n",
      "Step #1: Successfully installed Jinja2-2.10 MarkupSafe-1.1.1 Werkzeug-0.14.1 cachetools-3.1.0 certifi-2019.3.9 chardet-3.0.4 click-7.0 flask-1.0.2 google-api-core-0.1.4 google-auth-1.6.3 google-cloud-core-0.28.1 google-cloud-storage-1.6.0 google-resumable-media-0.3.2 googleapis-common-protos-1.5.8 gunicorn-19.9.0 idna-2.8 itsdangerous-1.1.0 numpy-1.16.2 pandas-0.24.1 protobuf-3.7.0 pyasn1-0.4.5 pyasn1-modules-0.2.4 python-dateutil-2.8.0 pytz-2018.9 requests-2.21.0 rsa-4.0 six-1.12.0 urllib3-1.24.1\n",
      "Step #1: \u001b[91mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "Step #1: You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\u001b[91m\n",
      "Step #1: \u001b[0mRemoving intermediate container 6376b4c2209b\n",
      "Step #1:  ---> 394ad384a61b\n",
      "Step #1: Step 8/9 : ADD . /app/\n",
      "Step #1:  ---> 55fcacfc3d8a\n",
      "Step #1: Step 9/9 : CMD exec gunicorn -b :$PORT main:app\n",
      "Step #1:  ---> Running in 19fdac393c67\n",
      "Step #1: Removing intermediate container 19fdac393c67\n",
      "Step #1:  ---> 0c471179df14\n",
      "Step #1: Successfully built 0c471179df14\n",
      "Step #1: Successfully tagged us.gcr.io/qwiklabs-gcp-a16809804f6b9f6f/appengine/default.20190309t205444:latest\n",
      "Finished Step #1\n",
      "PUSH\n",
      "Pushing us.gcr.io/qwiklabs-gcp-a16809804f6b9f6f/appengine/default.20190309t205444:latest\n",
      "The push refers to repository [us.gcr.io/qwiklabs-gcp-a16809804f6b9f6f/appengine/default.20190309t205444]\n",
      "6b826c598503: Preparing\n",
      "247c62931368: Preparing\n",
      "bc1f7c53c9ab: Preparing\n",
      "a061affdec5b: Preparing\n",
      "06e77ecee792: Preparing\n",
      "6835dde49809: Preparing\n",
      "10cbfff6b6ef: Preparing\n",
      "5ad1877af6b1: Preparing\n",
      "eb5b0f432768: Preparing\n",
      "819a72abea7c: Preparing\n",
      "d08d5b6a3090: Preparing\n",
      "a986e3689b44: Preparing\n",
      "84ff92691f90: Preparing\n",
      "5ea7858ba7ae: Preparing\n",
      "5016ea8d7f17: Preparing\n",
      "6835dde49809: Waiting\n",
      "10cbfff6b6ef: Waiting\n",
      "5ad1877af6b1: Waiting\n",
      "eb5b0f432768: Waiting\n",
      "819a72abea7c: Waiting\n",
      "d08d5b6a3090: Waiting\n",
      "a986e3689b44: Waiting\n",
      "84ff92691f90: Waiting\n",
      "5ea7858ba7ae: Waiting\n",
      "5016ea8d7f17: Waiting\n",
      "06e77ecee792: Layer already exists\n",
      "6835dde49809: Layer already exists\n",
      "10cbfff6b6ef: Layer already exists\n",
      "6b826c598503: Pushed\n",
      "bc1f7c53c9ab: Pushed\n",
      "5ad1877af6b1: Layer already exists\n",
      "eb5b0f432768: Layer already exists\n",
      "819a72abea7c: Layer already exists\n",
      "d08d5b6a3090: Layer already exists\n",
      "a986e3689b44: Layer already exists\n",
      "a061affdec5b: Pushed\n",
      "5ea7858ba7ae: Layer already exists\n",
      "5016ea8d7f17: Layer already exists\n",
      "84ff92691f90: Layer already exists\n",
      "247c62931368: Pushed\n",
      "latest: digest: sha256:c9397a80ac37555deb2b6f90d932cce40377b30de1f4116701edb722c2106f45 size: 3458\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Updating service [default] (this may take several minutes)...\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n",
      "Setting traffic split for service [default]...\n",
      ".......done.\n",
      "Deployed service [default] to [https://qwiklabs-gcp-a16809804f6b9f6f.appspot.com]\n",
      "\n",
      "You can stream logs from the command line by running:\n",
      "  $ gcloud app logs tail -s default\n",
      "\n",
      "To view your application in the web browser run:\n",
      "  $ gcloud app browse\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud -q app deploy app/app_template.yaml_deploy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take 7 - 10 minutes to deploy the app. While you wait, consider starting on Part Two below and completing the Cloud Composer DAG file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the API for Article Recommendations\n",
    "Lastly, you are able to test the recommendation model API by submitting a query request. Note the example userId passed and numRecs desired as the URL parameters for the model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl \"https://qwiklabs-gcp-a16809804f6b9f6f.appspot.com/recommendation?userId=5448543647176335931&numRecs=5\"\n",
      "{\"articles\":[\"299824032\",\"299865757\",\"1701682\",\"299959410\",\"299935287\"]}\n",
      "\n",
      "This command will exit automatically in 300 seconds.\n",
      "Generating traffic to https://qwiklabs-gcp-a16809804f6b9f6f.appspot.com/recommendation?userId=5448543647176335931&numRecs=5...\n",
      "Press Ctrl-C to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100    73  100    73    0     0    126      0 --:--:-- --:--:-- --:--:--   126\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd scripts\n",
    "./query_api.sh          # Query the API.\n",
    "./generate_traffic.sh   # Send traffic to the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the call is successful, you will see the article IDs recommended for that specific user by the WALS ML model <br/>\n",
    "(Example: curl \"https://qwiklabs-gcp-12345.appspot.com/recommendation?userId=5448543647176335931&numRecs=5\"\n",
    "{\"articles\":[\"299824032\",\"1701682\",\"299935287\",\"299959410\",\"298157062\"]} )\n",
    "\n",
    "__Part One is done!__ You have successfully created the back-end architecture for serving your ML recommendation system. But we're not done yet, we still need to automatically retrain and redeploy our model once new data comes in. For that we will use [Cloud Composer](https://cloud.google.com/composer/) and [Apache Airflow](https://airflow.apache.org/).<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part Two: Setup a scheduled workflow with Cloud Composer\n",
    "In this section you will complete a partially written training.py DAG file and copy it to the DAGS folder in your Composer instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy your Airflow bucket name\n",
    "1. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n",
    "2. Select __DAGs Folder__<br/><br/>\n",
    "3. You will be taken to the Google Cloud Storage bucket that Cloud Composer has created automatically for your Airflow instance<br/><br/>\n",
    "4. __Copy the bucket name__ into the variable below (example: us-central1-composer-08f6edeb-bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AIRFLOW_BUCKET = 'us-central1-mlcomposer-70c5ac80-bucket' # REPLACE WITH AIRFLOW BUCKET NAME\n",
    "os.environ['AIRFLOW_BUCKET'] = AIRFLOW_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the training.py DAG file\n",
    "Apache Airflow orchestrates tasks out to other services through a [DAG (Directed Acyclic Graph)](https://airflow.apache.org/concepts.html) file which specifies what services to call, what to do, and when to run these tasks. DAG files are written in python and are loaded automatically into Airflow once present in the Airflow/dags/ folder in your Cloud Composer bucket. \n",
    "\n",
    "Your task is to complete the partially written DAG file below which will enable the automatic retraining and redeployment of our WALS recommendation model. \n",
    "\n",
    "__Complete the #TODOs__ in the Airflow DAG file below and execute the code block to save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting airflow/dags/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile airflow/dags/training.py\n",
    "\n",
    "# Copyright 2018 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"DAG definition for recserv model training.\"\"\"\n",
    "\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "\n",
    "# Reference for all available airflow operators: \n",
    "# https://github.com/apache/incubator-airflow/tree/master/airflow/contrib/operators\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\n",
    "from airflow.hooks.base_hook import BaseHook\n",
    "# from airflow.contrib.operators.mlengine_operator import MLEngineTrainingOperator\n",
    "# above mlengine_operator currently doesnt support custom MasterType so we import our own plugins:\n",
    "\n",
    "# custom plugins\n",
    "from airflow.operators.app_engine_admin_plugin import AppEngineVersionOperator\n",
    "from airflow.operators.ml_engine_plugin import MLEngineTrainingOperator\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "def _get_project_id():\n",
    "  \"\"\"Get project ID from default GCP connection.\"\"\"\n",
    "\n",
    "  extras = BaseHook.get_connection('google_cloud_default').extra_dejson\n",
    "  key = 'extra__google_cloud_platform__project'\n",
    "  if key in extras:\n",
    "    project_id = extras[key]\n",
    "  else:\n",
    "    raise ('Must configure project_id in google_cloud_default '\n",
    "           'connection from Airflow Console')\n",
    "  return project_id\n",
    "\n",
    "PROJECT_ID = _get_project_id()\n",
    "\n",
    "# Data set constants, used in BigQuery tasks.  You can change these\n",
    "# to conform to your data.\n",
    "\n",
    "# TODO: Specify your BigQuery dataset name and table name\n",
    "DATASET = 'GA360_test'\n",
    "TABLE_NAME = 'ga_sessions_sample'\n",
    "ARTICLE_CUSTOM_DIMENSION = '10'\n",
    "\n",
    "# TODO: Confirm bucket name and region\n",
    "# GCS bucket names and region, can also be changed.\n",
    "BUCKET = 'gs://recserve_' + PROJECT_ID\n",
    "REGION = 'us-central1'\n",
    "\n",
    "# The code package name comes from the model code in the wals_ml_engine\n",
    "# directory of the solution code base.\n",
    "PACKAGE_URI = BUCKET + '/code/wals_ml_engine-0.1.tar.gz'\n",
    "JOB_DIR = BUCKET + '/jobs'\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 5,\n",
    "    'retry_delay': datetime.timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "# Default schedule interval using cronjob syntax - can be customized here\n",
    "# or in the Airflow console.\n",
    "\n",
    "# TODO: Specify a schedule interval in CRON syntax to run once a day at 2100 hours (9pm)\n",
    "# Reference: https://airflow.apache.org/scheduler.html\n",
    "schedule_interval = '0 21 * * *' # example '00 XX 0 0 0'\n",
    "\n",
    "# TODO: Title your DAG to be recommendations_training_v1\n",
    "dag = DAG('recommendations_training_v1', \n",
    "          default_args=default_args,\n",
    "          schedule_interval=schedule_interval)\n",
    "\n",
    "dag.doc_md = __doc__\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# Task Definition\n",
    "#\n",
    "#\n",
    "\n",
    "# BigQuery training data query\n",
    "\n",
    "bql='''\n",
    "#legacySql\n",
    "SELECT\n",
    " fullVisitorId as clientId,\n",
    " ArticleID as contentId,\n",
    " (nextTime - hits.time) as timeOnPage,\n",
    "FROM(\n",
    "  SELECT\n",
    "    fullVisitorId,\n",
    "    hits.time,\n",
    "    MAX(IF(hits.customDimensions.index={0},\n",
    "           hits.customDimensions.value,NULL)) WITHIN hits AS ArticleID,\n",
    "    LEAD(hits.time, 1) OVER (PARTITION BY fullVisitorId, visitNumber\n",
    "                             ORDER BY hits.time ASC) as nextTime\n",
    "  FROM [{1}.{2}.{3}]\n",
    "  WHERE hits.type = \"PAGE\"\n",
    ") HAVING timeOnPage is not null and contentId is not null;\n",
    "'''\n",
    "\n",
    "bql = bql.format(ARTICLE_CUSTOM_DIMENSION, PROJECT_ID, DATASET, TABLE_NAME)\n",
    "\n",
    "# TODO: Complete the BigQueryOperator task to truncate the table if it already exists before writing\n",
    "# Reference: https://airflow.apache.org/integration.html#bigqueryoperator\n",
    "t1 = BigQueryOperator( # correct the operator name\n",
    "    task_id='bq_rec_training_data',\n",
    "    bql=bql,\n",
    "    destination_dataset_table='%s.recommendation_events' % DATASET,\n",
    "    write_disposition='WRITE_TRUNCATE', # specify to truncate on writes\n",
    "    dag=dag)\n",
    "\n",
    "# BigQuery training data export to GCS\n",
    "\n",
    "# TODO: Fill in the missing operator name for task #2 which\n",
    "# takes a BigQuery dataset and table as input and exports it to GCS as a CSV\n",
    "training_file = BUCKET + '/data/recommendation_events.csv'\n",
    "t2 = BigQueryToCloudStorageOperator( # correct the name\n",
    "    task_id='bq_export_op',\n",
    "    source_project_dataset_table='%s.recommendation_events' % DATASET,\n",
    "    destination_cloud_storage_uris=[training_file],\n",
    "    export_format='CSV',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "# ML Engine training job\n",
    "\n",
    "job_id = 'recserve_{0}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M'))\n",
    "job_dir = BUCKET + '/jobs/' + job_id\n",
    "output_dir = BUCKET\n",
    "training_args = ['--job-dir', job_dir,\n",
    "                 '--train-files', training_file,\n",
    "                 '--output-dir', output_dir,\n",
    "                 '--data-type', 'web_views',\n",
    "                 '--use-optimized']\n",
    "\n",
    "# TODO: Fill in the missing operator name for task #3 which will\n",
    "# start a new training job to Cloud ML Engine\n",
    "# Reference: https://airflow.apache.org/integration.html#cloud-ml-engine\n",
    "# https://cloud.google.com/ml-engine/docs/tensorflow/machine-types\n",
    "t3 = MLEngineTrainingOperator( # complete the name\n",
    "    task_id='ml_engine_training_op',\n",
    "    project_id=PROJECT_ID,\n",
    "    job_id=job_id,\n",
    "    package_uris=[PACKAGE_URI],\n",
    "    training_python_module='trainer.task',\n",
    "    training_args=training_args,\n",
    "    region=REGION,\n",
    "    scale_tier='CUSTOM',\n",
    "    master_type='complex_model_m_gpu',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# App Engine deploy new version\n",
    "\n",
    "t4 = AppEngineVersionOperator(\n",
    "    task_id='app_engine_deploy_version',\n",
    "    project_id=PROJECT_ID,\n",
    "    service_id='default',\n",
    "    region=REGION,\n",
    "    service_spec=None,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# TODO: Be sure to set_upstream dependencies for all tasks\n",
    "t2.set_upstream(t1)\n",
    "t3.set_upstream(t2)\n",
    "t4.set_upstream(t3) # complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy local Airflow DAG file and plugins into the DAGs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://airflow/dags/training.py [Content-Type=text/x-python]...\n",
      "/ [0 files][    0.0 B/  6.0 KiB]                                                \r",
      "/ [1 files][  6.0 KiB/  6.0 KiB]                                                \r\n",
      "Operation completed over 1 objects/6.0 KiB.                                      \n",
      "Copying file://airflow/plugins/gae_admin_plugin.py [Content-Type=text/x-python]...\n",
      "/ [0 files][    0.0 B/ 10.0 KiB]                                                \r",
      "/ [1 files][ 10.0 KiB/ 10.0 KiB]                                                \r",
      "Copying file://airflow/plugins/ml_engine_plugin.py [Content-Type=text/x-python]...\n",
      "/ [1 files][ 10.0 KiB/ 17.9 KiB]                                                \r",
      "/ [2 files][ 17.9 KiB/ 17.9 KiB]                                                \r\n",
      "Operation completed over 2 objects/17.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp airflow/dags/training.py gs://${AIRFLOW_BUCKET}/dags # overwrite if it exists\n",
    "gsutil cp -r airflow/plugins gs://${AIRFLOW_BUCKET} # copy custom plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n",
    "\n",
    "3. Trigger a __manual run__ of your DAG for testing<br/><br/>\n",
    "\n",
    "3. Ensure your DAG runs successfully (all nodes outlined in dark green and 'success' tag shows)\n",
    "\n",
    "![Successful Airflow DAG run](./img/airflow_successful_run.jpg \"Successful Airflow DAG run\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting your DAG\n",
    "\n",
    "DAG not executing successfully? Follow these below steps to troubleshoot.\n",
    "\n",
    "Click on the name of a DAG to view a run (ex: recommendations_training_v1)\n",
    "\n",
    "1. Select a node in the DAG (red or yellow borders mean failed nodes)\n",
    "2. Select View Log\n",
    "3. Scroll to the bottom of the log to diagnose\n",
    "4. X Option: Clear and immediately restart the DAG after diagnosing the issue\n",
    "\n",
    "Tips:\n",
    "- If bq_rec_training_data immediately fails without logs, your DAG file is missing key parts and is not compiling\n",
    "- ml_engine_training_op will take 9 - 12 minutes to run. Monitor the training job in [ML Engine](https://console.cloud.google.com/mlengine/jobs?project=)\n",
    "- Lastly, check the [solution endtoend.ipynb](../endtoend/endtoend.ipynb) to compare your lab answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Viewing Airflow logs](./img/airflow_viewing_logs.jpg \"Viewing Airflow logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "You have made it to the end of the end-to-end recommendation system lab. You have successfully setup an automated workflow to retrain and redeploy your recommendation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Challenges\n",
    "\n",
    "Looking to solidify your Cloud Composer skills even more? Complete the __optional challenges__ below\n",
    "<br/><br/>\n",
    "### Challenge 1\n",
    "Use either the [BigQueryCheckOperator](https://airflow.apache.org/integration.html#bigquerycheckoperator) or the [BigQueryValueCheckOperator](https://airflow.apache.org/integration.html#bigqueryvaluecheckoperator) to create a new task in your DAG that ensures the SQL query for training data is returning valid results before it is passed to Cloud ML Engine for training. \n",
    "<br/><br/>\n",
    "Hint: Check for COUNT() = 0 or other health check\n",
    "<br/><br/><br/>\n",
    "### Challenge 2\n",
    "Create a Cloud Function to [automatically trigger](https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf) your DAG when a new recommendation_events.csv file is loaded into your Google Cloud Storage Bucket. \n",
    "<br/><br/>\n",
    "Hint: Check the [composer_gcf_trigger.ipynb lab](../composer_gcf_trigger/composertriggered.ipynb) for inspiration\n",
    "<br/><br/><br/>\n",
    "### Challenge 3\n",
    "Modify the BigQuery query in the DAG to only train on a portion of the data available in the dataset using a WHERE clause filtering on date. Next, parameterize the WHERE clause to be based on when the Airflow DAG is run\n",
    "<br/><br/>\n",
    "Hint: Make use of prebuilt [Airflow macros](https://airflow.incubator.apache.org/_modules/airflow/macros.html) like the below:\n",
    "\n",
    "_constants or can be dynamic based on Airflow macros_ <br/>\n",
    "max_query_date = '2018-02-01' # {{ macros.ds_add(ds, -7) }} <br/>\n",
    "min_query_date = '2018-01-01' # {{ macros.ds_add(ds, -1) }} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Follow the latest [Airflow operators](https://github.com/apache/incubator-airflow/tree/master/airflow/contrib/operators) on github"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
